# Proyecto VLM: BLIP, CLIP y FLAVA

Este repositorio contiene scripts para trabajar con **modelos de visiÃ³n-lenguaje (VLM)**: BLIP, CLIP y FLAVA, aplicados a un dataset propio de imÃ¡genes y captions.

Los modelos se ejecutan dentro de un contenedor Docker, y se proporcionan recetas de Makefile para facilitar la ejecuciÃ³n.

---

## ğŸ“‚ Estructura del repositorio

/proyecto
â”œâ”€ informes/ â† Informes y documentaciÃ³n
â”œâ”€ clip/
â”‚ â”œâ”€ dataset/ â† ImÃ¡genes y CSV de captions
â”‚ â”œâ”€ blip.py â† EvaluaciÃ³n BLIP (ITM)
â”‚ â”œâ”€ blip2.py â† GeneraciÃ³n de captions con BLIP
â”‚ â”œâ”€ clip.py â† Ejemplo CLIP
â”‚ â”œâ”€ clip_laion.py â† CLIP Laion
â”‚ â”œâ”€ pre-clip.py â† Preprocesamiento para CLIP
â”‚ â”œâ”€ flava.py â† EvaluaciÃ³n FLAVA
â”‚ â”œâ”€ flava2.py â† EvaluaciÃ³n FLAVA (multimodal)
â”‚ â”œâ”€ Dockerfile â† Imagen Docker con dependencias
â”‚ â””â”€ Makefile â† Recetas para construir y ejecutar scripts


---

## âš¡ Requisitos

- Docker â‰¥ 20.10  
- (Opcional) GPU compatible con CUDA para acelerar inferencia  
- X11 para ejecutar GUIs dentro del contenedor (solo si es necesario)

---

## ğŸ— ConstrucciÃ³n de la imagen

Desde la carpeta `clip/`:

```bash
make build
